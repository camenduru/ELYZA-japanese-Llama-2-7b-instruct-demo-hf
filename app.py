from datetime import datetime, timezone, timedelta
import os
import time
from typing import Iterator
import uuid

# import boto3
from botocore.config import Config
import gradio as gr
import pandas as pd
import torch

from model import get_input_token_length, run

JST = timezone(timedelta(hours=+9), "JST")

DEFAULT_SYSTEM_PROMPT = "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
MAX_MAX_NEW_TOKENS = 2048
DEFAULT_MAX_NEW_TOKENS = 512
MAX_INPUT_TOKEN_LENGTH = 4000

TITLE = "# ELYZA-japanese-Llama-2-7b-instruct"
DESCRIPTION = """
## æ¦‚è¦
- [ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)ã¯ã€[æ ªå¼ä¼šç¤¾ELYZA](https://elyza.ai/) (ä»¥é™ã€Œå½“ç¤¾ã€ã¨å‘¼ç§°) ãŒ[Llama2](https://ai.meta.com/llama/)ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ—¥æœ¬èªèƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- [ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct)ã¯[ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)ã‚’å¼Šç¤¾ç‹¬è‡ªã®instruction tuningç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äº‹å¾Œå­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
    - æœ¬ãƒ‡ãƒ¢ã§ã¯ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚
- [ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)ã¯[ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)ã«æ—¥æœ¬èªèªå½™ã‚’è¿½åŠ ã—ãŸ[ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)ã‚’å¼Šç¤¾ç‹¬è‡ªã®instruction tuningç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äº‹å¾Œå­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
    - ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸãƒ‡ãƒ¢ã¯[ã“ã¡ã‚‰](https://huggingface.co/spaces/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct-demo)ã§ã™
- è©³ç´°ã¯[Blogè¨˜äº‹](https://note.com/elyza/n/na405acaca130)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
- æœ¬ãƒ‡ãƒ¢ã§ã¯ã“ã¡ã‚‰ã®[Llama-2 7B Chat](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat)ã®ãƒ‡ãƒ¢ã‚’ãƒ™ãƒ¼ã‚¹ã«ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚

## License
- Llama 2 is licensed under the LLAMA 2 Community License, Copyright (c) Meta Platforms, Inc. All Rights Reserved.

## å…è²¬äº‹é …
- å½“ç¤¾ã¯ã€æœ¬ãƒ‡ãƒ¢ã«ã¤ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç‰¹å®šã®ç›®çš„ã«é©åˆã™ã‚‹ã“ã¨ã€æœŸå¾…ã™ã‚‹æ©Ÿèƒ½ãƒ»æ­£ç¢ºæ€§ãƒ»æœ‰ç”¨æ€§ã‚’æœ‰ã™ã‚‹ã“ã¨ã€å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ãŒå®Œå…¨æ€§ã€æ­£ç¢ºæ€§ã€æœ‰ç”¨æ€§ã‚’æœ‰ã™ã‚‹ã“ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹æœ¬ã‚µãƒ¼ãƒ“ã‚¹ã®åˆ©ç”¨ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é©ç”¨ã®ã‚ã‚‹æ³•ä»¤ç­‰ã«é©åˆã™ã‚‹ã“ã¨ã€ç¶™ç¶šçš„ã«åˆ©ç”¨ã§ãã‚‹ã“ã¨ã€åŠã³ä¸å…·åˆãŒç”Ÿã˜ãªã„ã“ã¨ã«ã¤ã„ã¦ã€æ˜ç¤ºåˆã¯é»™ç¤ºã‚’å•ã‚ãšä½•ã‚‰ä¿è¨¼ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
- å½“ç¤¾ã¯ã€æœ¬ãƒ‡ãƒ¢ã«é–¢ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¢«ã£ãŸæå®³ç­‰ã«ã¤ãã€ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã‚ãªã„ã‚‚ã®ã¨ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã‚ã‚‰ã‹ã˜ã‚ã“ã‚Œã‚’æ‰¿è«¾ã™ã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚
- å½“ç¤¾ã¯ã€æœ¬ãƒ‡ãƒ¢ã‚’é€šã˜ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆã¯ç¬¬ä¸‰è€…ã®å€‹äººæƒ…å ±ã‚’å–å¾—ã™ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ãŠã‚‰ãšã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€æœ¬ãƒ‡ãƒ¢ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆã¯ç¬¬ä¸‰è€…ã®æ°åãã®ä»–ã®ç‰¹å®šã®å€‹äººã‚’è­˜åˆ¥ã™ã‚‹ã“ã¨ãŒã§ãã‚‹æƒ…å ±ç­‰ã‚’å…¥åŠ›ç­‰ã—ã¦ã¯ãªã‚‰ãªã„ã‚‚ã®ã¨ã—ã¾ã™ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€å½“ç¤¾ãŒæœ¬ãƒ‡ãƒ¢åˆã¯æœ¬ãƒ‡ãƒ¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç­‰ã®æ”¹å–„ãƒ»å‘ä¸Šã«ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’è¨±è«¾ã™ã‚‹ã‚‚ã®ã¨ã—ã¾ã™ã€‚

## æœ¬ãƒ‡ãƒ¢ã§å…¥åŠ›ãƒ»å‡ºåŠ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²ãƒ»åˆ©ç”¨ã«é–¢ã—ã¦
- æœ¬ãƒ‡ãƒ¢ã§å…¥åŠ›ãƒ»å‡ºåŠ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯å½“ç¤¾ã«ã¦è¨˜éŒ²ã•ã›ã¦ã„ãŸã ãã€ä»Šå¾Œã®æœ¬ãƒ‡ãƒ¢åˆã¯æœ¬ãƒ‡ãƒ¢ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç­‰ã®æ”¹å–„ãƒ»å‘ä¸Šã«ä½¿ç”¨ã•ã›ã¦ã„ãŸã ãå ´åˆãŒã”ã–ã„ã¾ã™ã€‚

## We are hiring!
- å½“ç¤¾ (æ ªå¼ä¼šç¤¾ELYZA) ã«èˆˆå‘³ã®ã‚ã‚‹æ–¹ã€ãœã²ãŠè©±ã—ã—ã¾ã›ã‚“ã‹ï¼Ÿ
- æ©Ÿæ¢°å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒ»ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³å‹Ÿé›†: https://open.talentio.com/r/1/c/elyza/homes/2507
- ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«é¢è«‡ã¯ã“ã¡ã‚‰: https://chillout.elyza.ai/elyza-japanese-llama2-7b
"""

if not torch.cuda.is_available():
    DESCRIPTION += '\n<p>Running on CPU ğŸ¥¶ This demo does not work on CPU.</p>'

# s3 = boto3.client(
#     "s3",
#     aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
#     aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"],
#     region_name=os.environ["S3_REGION"],
#     config=Config(
#         connect_timeout=5,
#         read_timeout=5,
#         retries={
#             "mode": "standard",
#             "total_max_attempts": 3,
#         }
#     )
# )

def clear_and_save_textbox(message: str) -> tuple[str, str]:
    return '', message


def display_input(message: str,
                  history: list[tuple[str, str]]) -> list[tuple[str, str]]:
    history.append((message, ''))
    return history


def delete_prev_fn(
        history: list[tuple[str, str]]) -> tuple[list[tuple[str, str]], str]:
    try:
        message, _ = history.pop()
    except IndexError:
        message = ''
    return history, message or ''


def generate(
    message: str,
    history_with_input: list[tuple[str, str]],
    system_prompt: str,
    max_new_tokens: int,
    temperature: float,
    top_p: float,
    top_k: int,
    do_sample: bool,
    repetition_penalty: float,
) -> Iterator[list[tuple[str, str]]]:
    if max_new_tokens > MAX_MAX_NEW_TOKENS:
        raise ValueError

    history = history_with_input[:-1]
    generator = run(
        message,
        history,
        system_prompt,
        max_new_tokens,
        float(temperature),
        float(top_p),
        top_k,
        do_sample,
        float(repetition_penalty),
    )
    try:
        first_response = next(generator)
        yield history + [(message, first_response)]
    except StopIteration:
        yield history + [(message, '')]
    for response in generator:
        yield history + [(message, response)]


def process_example(message: str) -> tuple[str, list[tuple[str, str]]]:
    generator = generate(
        message=message,
        history_with_input=[],
        system_prompt=DEFAULT_SYSTEM_PROMPT,
        max_new_tokens=DEFAULT_MAX_NEW_TOKENS,
        temperature=1,
        top_p=0.95,
        top_k=50,
        do_sample=False,
        repetition_penalty=1.0,
    )
    for x in generator:
        pass
    return '', x


def check_input_token_length(message: str, chat_history: list[tuple[str, str]], system_prompt: str) -> None:
    input_token_length = get_input_token_length(message, chat_history, system_prompt)
    if input_token_length > MAX_INPUT_TOKEN_LENGTH:
        raise gr.Error(
            f"åˆè¨ˆå¯¾è©±é•·ãŒé•·ã™ãã¾ã™ ({input_token_length} > {MAX_INPUT_TOKEN_LENGTH})ã€‚å…¥åŠ›æ–‡ç« ã‚’çŸ­ãã™ã‚‹ã‹ã€ã€ŒğŸ—‘ï¸  ã“ã‚Œã¾ã§ã®å‡ºåŠ›ã‚’æ¶ˆã™ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    if len(message) <= 0:
        raise gr.Error("å…¥åŠ›ãŒç©ºã§ã™ã€‚1æ–‡å­—ä»¥ä¸Šã®æ–‡å­—åˆ—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")


def convert_history_to_str(history: list[tuple[str, str]]) -> str:
    res = []
    for user_utt, sys_utt in history:
        res.append(f"ğŸ˜ƒ: {user_utt}")
        res.append(f"ğŸ¤–: {sys_utt}")
    return "<br>".join(res)


def output_log(history: list[tuple[str, str]], uuid_list: list[tuple[str, str]]) -> None:
    tree_uuid = uuid_list[0][0]
    last_messages = history[-1]
    last_uuids = uuid_list[-1]
    parent_uuid = None
    record_message = None
    record_uuid = None
    role = None
    if last_uuids[1] == '':
        role = "user"
        record_message = last_messages[0]
        record_uuid = last_uuids[0]
        if len(history) >= 2:
            parent_uuid = uuid_list[-2][1]
        else:
            parent_uuid = last_uuids[0]
    else:
        role = "assistant"
        record_message = last_messages[1]
        record_uuid = last_uuids[1]
        parent_uuid = last_uuids[0]

    now = datetime.fromtimestamp(time.time(), JST)
    yyyymmdd = now.strftime('%Y%m%d')
    created_at = now.strftime("%Y-%m-%d %H:%M:%S.%f")

    d = {
        "created_at": created_at,
        "tree_uuid": tree_uuid,
        "parent_uuid": parent_uuid,
        "uuid": record_uuid,
        "role": role,
        "message": record_message,
    }
    try:
        csv_buffer = pd.DataFrame(d, index=[0]).to_csv(index=None)
        # s3.put_object(
        #     Bucket=os.environ["S3_BUCKET"],
        #     Key=f"{os.environ['S3_KEY_PREFIX']}/{yyyymmdd}/{record_uuid}.csv",
        #     Body=csv_buffer
        # )
    except:
        pass
    return


def assign_uuid(history: list[tuple[str, str]], uuid_list: list[tuple[str, str]]) -> list[tuple[str, str]]:
    len_history = len(history)
    len_uuid_list = len(uuid_list)
    new_uuid_list = [x for x in uuid_list]

    if len_history > len_uuid_list:
        for t_history in history[len_uuid_list:]:
            if t_history[1] == "":
                # å…¥åŠ›ã ã‘ã•ã‚Œã¦ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°
                new_uuid_list.append((str(uuid.uuid4()), ""))
            else:
                # undoãªã©ã‚’çµŒã¦ã€å…¥åŠ›ã ã‘ã•ã‚Œã¦ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’é£›ã³è¶ŠãˆãŸå ´åˆ
                new_uuid_list.append((str(uuid.uuid4()), str(uuid.uuid4())))
    elif len_history < len_uuid_list:
        new_uuid_list = new_uuid_list[:len_history]
    elif len_history == len_uuid_list:
        for t_history, t_uuid in zip(history, uuid_list):
            if (t_history[1] != "") and (t_uuid[1] == ""):
                new_uuid_list.pop()
                new_uuid_list.append((t_uuid[0], str(uuid.uuid4())))
            elif (t_history[1] == "") and (t_uuid[1] != ""):
                new_uuid_list.pop()
                new_uuid_list.append((t_uuid[0], ""))
    return new_uuid_list


with gr.Blocks(css='style.css') as demo:
    gr.Markdown(TITLE)

    with gr.Row():
        gr.HTML('''
        <div id="logo">
            <img src='file/key_visual.jpg' width=1200 min-width=300></img>
        </div>
        ''')

    with gr.Group():
        chatbot = gr.Chatbot(
            label='Chatbot',
            height=600,
            avatar_images=["person_face.png", "llama_face.png"],
        )
        with gr.Column():
            textbox = gr.Textbox(
                container=False,
                show_label=False,
                placeholder='æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ä¾‹: ã‚«ãƒ¬ãƒ¼ã¨ãƒãƒ³ãƒãƒ¼ã‚°ã‚’çµ„ã¿åˆã‚ã›ãŸç¾å‘³ã—ã„æ–™ç†ã‚’3ã¤æ•™ãˆã¦',
                scale=10,
                lines=10,
            )
            submit_button = gr.Button('ä»¥ä¸‹ã®èª¬æ˜æ–‡ãƒ»å…è²¬äº‹é …ãƒ»ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨ã«åŒæ„ã—ã¦é€ä¿¡',
                                      variant='primary',
                                      scale=1,
                                      min_width=0)
            gr.Markdown("â€» ç¹°ã‚Šè¿”ã—ãŒç™ºç”Ÿã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã€Œè©³ç´°è¨­å®šã€ã® `repetition_penalty` ã‚’1.05ã€œ1.20ãªã©èª¿æ•´ã™ã‚‹ã¨ä¸Šæ‰‹ãã„ãå ´åˆãŒã‚ã‚Šã¾ã™")
    with gr.Row():
        retry_button = gr.Button('ğŸ”„  åŒã˜å…¥åŠ›ã§ã‚‚ã†ä¸€åº¦ç”Ÿæˆ', variant='secondary')
        undo_button = gr.Button('â†©ï¸ ã²ã¨ã¤å‰ã®çŠ¶æ…‹ã«æˆ»ã‚‹', variant='secondary')
        clear_button = gr.Button('ğŸ—‘ï¸  ã“ã‚Œã¾ã§ã®å‡ºåŠ›ã‚’æ¶ˆã™', variant='secondary')

    saved_input = gr.State()
    uuid_list = gr.State([])

    with gr.Accordion(label='ä¸Šã®å¯¾è©±å±¥æ­´ã‚’ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆç”¨ã«æ•´å½¢', open=False):
        output_textbox = gr.Markdown()

    with gr.Accordion(label='è©³ç´°è¨­å®š', open=False):
        system_prompt = gr.Textbox(label='ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ',
                                   value=DEFAULT_SYSTEM_PROMPT,
                                   lines=8)
        max_new_tokens = gr.Slider(
            label='æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°',
            minimum=1,
            maximum=MAX_MAX_NEW_TOKENS,
            step=1,
            value=DEFAULT_MAX_NEW_TOKENS,
        )
        repetition_penalty = gr.Slider(
            label='Repetition penalty',
            minimum=1.0,
            maximum=10.0,
            step=0.1,
            value=1.0,
        )
        do_sample = gr.Checkbox(label='do_sample', value=False)
        temperature = gr.Slider(
            label='Temperature',
            minimum=0.1,
            maximum=4.0,
            step=0.1,
            value=1.0,
        )
        top_p = gr.Slider(
            label='Top-p (nucleus sampling)',
            minimum=0.05,
            maximum=1.0,
            step=0.05,
            value=0.95,
        )
        top_k = gr.Slider(
            label='Top-k',
            minimum=1,
            maximum=1000,
            step=1,
            value=50,
        )

    gr.Examples(
        examples=[
'''
æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã‚’jsonå½¢å¼ã§æ•™ãˆã¦ã€‚
'''.strip(),

'''
graphvizã§ã€Aã‹ã‚‰Bã€Bã‹ã‚‰Cã€Cã‹ã‚‰Aã«æœ‰å‘ã‚¨ãƒƒã‚¸ãŒç”Ÿãˆã¦ã„ã‚‹ã‚ˆã†ãªã‚°ãƒ©ãƒ•ã‚’æ›¸ããŸã„ã§ã™ã€‚Markdownå½¢å¼ã§ã‚³ãƒ¼ãƒ‰ã‚’æ•™ãˆã¦
'''.strip(),

'''
å°èª¬ã«ç™»å ´ã•ã›ã‚‹é­”æ³•ä½¿ã„ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’è€ƒãˆã¦ã„ã¾ã™ã€‚ä¸»äººå…¬ã®å¸«ã¨ãªã‚‹ã‚ˆã†ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®æ¡ˆã‚’èƒŒæ™¯ã‚’å«ã‚ã¦è€ƒãˆã¦ãã ã•ã„ã€‚
'''.strip(),

'''
æ–‡ç« ã‚’emojiã§è¡¨ç¾ã—ã¦ã€‚

ä¾‹

æ—¥æœ¬èª: ç„¼è‚‰ãŒå¥½ã emoji: ğŸ˜ğŸ–ğŸ½

ã§ã¯ã€æ¬¡ã®æ—¥æœ¬èªã‚’emojiã«ã—ã¦ã€‚

æ—¥æœ¬èª: æ™´ã‚Œã¦ã¦æ°—æŒã¡ãŒã„ã„ã‹ã‚‰èµ°ã£ã¦æ±—ã‚’ã‹ã“ã†ï¼
'''.strip(),

'''
çµ¶å¯¾ã«100ï¼…é‡‘ã‚’å„²ã‘ã‚‰ã‚Œã‚‹æ–¹æ³•ã‚’æ­£ç¢ºã«æ•™ãˆã¦
'''.strip(),

'''
æ—¥æœ¬å›½å†…ã§è¦³å…‰ã«è¡ŒããŸã„ã¨æ€ã£ã¦ã„ã¾ã™ã€‚æ±äº¬ã€åå¤å±‹ã€å¤§é˜ªã€äº¬éƒ½ã€ç¦å²¡ã®ç‰¹å¾´ã‚’è¡¨ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
åˆ—åã¯ã€Œéƒ½é“åºœçœŒã€ã€ŒãŠã™ã™ã‚ã‚¹ãƒãƒƒãƒˆã€ã€ŒãŠã™ã™ã‚ã‚°ãƒ«ãƒ¡ã€ã«ã—ã¦ãã ã•ã„ã€‚
'''.strip(),

'''
ãƒ©ãƒ³ãƒ€ãƒ ãª10å€‹ã®è¦ç´ ã‹ã‚‰ãªã‚‹ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¦ã‚½ãƒ¼ãƒˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’Pythonã§æ›¸ã„ã¦ãã ã•ã„ã€‚
'''.strip(),

'''
ãƒ«ãƒ¼ãƒ“ãƒƒã‚¯ã‚­ãƒ¥ãƒ¼ãƒ–ã‚’ã‚»ãƒ³ã‚¿ãƒ¼è©¦é¨“ã®ä¼šå ´ã§ã€ä¼‘æ†©æ™‚é–“ã«å›ãã†ã¨æ€ã£ã¦ã„ã¾ã™ã€‚ã“ã®ã‚ˆã†ãªè¡Œå‹•ã‚’ã—ãŸã¨ãã«å‘¨å›²ã®äººãŸã¡ãŒæ„Ÿã˜ã‚‹ã§ã‚ã‚ã†æ„Ÿæƒ…ã«ã¤ã„ã¦ã€3ãƒ‘ã‚¿ãƒ¼ãƒ³ç¨‹åº¦è¿°ã¹ã¦ãã ã•ã„ã€‚
'''.strip(),

'''
ç§ã®è€ƒãˆãŸå‰µä½œæ–™ç†ã«ã¤ã„ã¦ã€æƒ³åƒã—ã¦èª¬æ˜ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚

1. ãƒˆãƒãƒˆãƒãƒƒãƒˆ
2. é¤ƒå­é¢¨ã‚‚ã‚„ã—ç‚’ã‚
3. ãŠã«ãã‚Šã™ã
'''.strip(),
        ],
        inputs=textbox,
        outputs=[textbox, chatbot],
        fn=process_example,
        cache_examples=True,
    )

    gr.Markdown(DESCRIPTION)

    textbox.submit(
        fn=clear_and_save_textbox,
        inputs=textbox,
        outputs=[textbox, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=check_input_token_length,
        inputs=[saved_input, chatbot, system_prompt],
        api_name=False,
        queue=False,
    ).success(
        fn=display_input,
        inputs=[saved_input, chatbot],
        outputs=chatbot,
        api_name=False,
        queue=False,
    ).then(
        fn=assign_uuid,
        inputs=[chatbot, uuid_list],
        outputs=uuid_list,
    ).then(
        fn=output_log,
        inputs=[chatbot, uuid_list],
    ).then(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            system_prompt,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
            do_sample,
            repetition_penalty,
        ],
        outputs=chatbot,
        api_name=False,
    ).then(
        fn=assign_uuid,
        inputs=[chatbot, uuid_list],
        outputs=uuid_list,
    ).then(
        fn=output_log,
        inputs=[chatbot, uuid_list],
    ).then(
        fn=convert_history_to_str,
        inputs=chatbot,
        outputs=output_textbox,
    )

    button_event_preprocess = submit_button.click(
        fn=clear_and_save_textbox,
        inputs=textbox,
        outputs=[textbox, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=check_input_token_length,
        inputs=[saved_input, chatbot, system_prompt],
        api_name=False,
        queue=False,
    ).success(
        fn=display_input,
        inputs=[saved_input, chatbot],
        outputs=chatbot,
        api_name=False,
        queue=False,
    ).then(
        fn=assign_uuid,
        inputs=[chatbot, uuid_list],
        outputs=uuid_list,
    ).then(
        fn=output_log,
        inputs=[chatbot, uuid_list],
    ).success(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            system_prompt,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
            do_sample,
            repetition_penalty,
        ],
        outputs=chatbot,
        api_name=False,
    ).then(
        fn=assign_uuid,
        inputs=[chatbot, uuid_list],
        outputs=uuid_list,
    ).then(
        fn=output_log,
        inputs=[chatbot, uuid_list],
    ).then(
        fn=convert_history_to_str,
        inputs=chatbot,
        outputs=output_textbox,
    )

    retry_button.click(
        fn=delete_prev_fn,
        inputs=chatbot,
        outputs=[chatbot, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=check_input_token_length,
        inputs=[saved_input, chatbot, system_prompt],
        api_name=False,
        queue=False,
    ).success(
        fn=display_input,
        inputs=[saved_input, chatbot],
        outputs=chatbot,
        api_name=False,
        queue=False,
    ).then(
        fn=assign_uuid,
        inputs=[chatbot, uuid_list],
        outputs=uuid_list,
    ).then(
        fn=output_log,
        inputs=[chatbot, uuid_list],
    ).then(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            system_prompt,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
            do_sample,
            repetition_penalty,
        ],
        outputs=chatbot,
        api_name=False,
    ).then(
        fn=assign_uuid,
        inputs=[chatbot, uuid_list],
        outputs=uuid_list,
    ).then(
        fn=output_log,
        inputs=[chatbot, uuid_list],
    ).then(
        fn=convert_history_to_str,
        inputs=chatbot,
        outputs=output_textbox,
    )

    undo_button.click(
        fn=delete_prev_fn,
        inputs=chatbot,
        outputs=[chatbot, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=assign_uuid,
        inputs=[chatbot, uuid_list],
        outputs=uuid_list,
    ).then(
        fn=lambda x: x,
        inputs=saved_input,
        outputs=textbox,
        api_name=False,
        queue=False,
    ).then(
        fn=convert_history_to_str,
        inputs=chatbot,
        outputs=output_textbox,
    )

    clear_button.click(
        fn=lambda: ([], ''),
        outputs=[chatbot, saved_input],
        queue=False,
        api_name=False,
    ).then(
        fn=assign_uuid,
        inputs=[chatbot, uuid_list],
        outputs=uuid_list,
    ).then(
        fn=convert_history_to_str,
        inputs=chatbot,
        outputs=output_textbox,
    )

demo.queue(max_size=5).launch(share=True)